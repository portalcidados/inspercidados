---
title: "Research Guidelines"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{research-guidelines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Research & Coding Guidelines

> Best practices for reproducible urban economics research at Insper Cidades

## Table of Contents
1. [Project Organization](#project-organization)
2. [Modern R Conventions](#modern-r-conventions)
3. [Data Access Tools](#data-access-tools)
4. [AI-Assisted Research](#ai-assisted-research)
5. [Insper Resources](#insper-resources)
6. [Essential References](#essential-references)
7. [Reproducibility Checklist](#reproducibility-checklist)

---

## Project Organization

### Recommended Project Structure

```
your_project/
â”œâ”€â”€ README.md              # Project overview and instructions
â”œâ”€â”€ .gitignore            # Git ignore file
â”œâ”€â”€ renv.lock             # Package versions for reproducibility
â”œâ”€â”€ your_project.Rproj    # RStudio project file (optional)
â”‚
â”œâ”€â”€ data/                 # Data files (git-ignored if large)
â”‚   â”œâ”€â”€ raw/             # Original, immutable data
â”‚   â”œâ”€â”€ processed/       # Cleaned data
â”‚   â””â”€â”€ final/           # Analysis-ready data
â”‚
â”œâ”€â”€ R/                    # R scripts
â”‚   â”œâ”€â”€ 01_download.R    # Data acquisition
â”‚   â”œâ”€â”€ 02_clean.R       # Data cleaning
â”‚   â”œâ”€â”€ 03_analyze.R     # Analysis
â”‚   â””â”€â”€ utils.R          # Helper functions
â”‚
â”œâ”€â”€ notebooks/           # Exploratory analysis (Quarto/RMarkdown)
â”‚   â””â”€â”€ exploratory.qmd
â”‚
â”œâ”€â”€ outputs/             # Results
â”‚   â”œâ”€â”€ figures/        # Graphs and plots
â”‚   â”œâ”€â”€ tables/         # Tables and summaries
â”‚   â””â”€â”€ reports/        # Final reports/papers
â”‚
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ codebook.md     # Variable descriptions
â”‚   â””â”€â”€ methods.md      # Methodology
â”‚
â””â”€â”€ tests/              # Unit tests for functions
    â””â”€â”€ test_utils.R
```

### File Naming Conventions

```r
# Good examples:
01_download_iptu.R
02_clean_census_2010.R
03_merge_datasets.R
04_regression_analysis.R

# Avoid:
Script1.R
analysis_FINAL_v2_FINAL.R
untitled.R
```

### Version Control Best Practices

```bash
# Good commit messages
git commit -m "Add IPTU cleaning pipeline for 2020-2024"
git commit -m "Fix encoding issues in address fields"

# Bad commit messages
git commit -m "Updates"
git commit -m "asdfasdf"
```

Always use `.gitignore` for:
- Large data files (use Git LFS or external storage)
- Credentials and API keys
- Operating system files (.DS_Store, Thumbs.db)
- R session files (.Rdata, .Rhistory)

---

## Modern R Conventions

### Choose Your Framework

For most users, we recommend the `tidyverse` for its readability and extensive ecosystem. For very large datasets, `data.table` is a strong alternative, but also consider `dtplyr` for a hybrid approach.

#### Option 1: Tidyverse (Recommended for most users)

The tidyverse provides a consistent and user-friendly syntax for data manipulation and visualization.

```r
# Modern, readable, pipe-based workflow
library(dplyr)
library(tidyr)
library(stringr)

# Prefer modern functions dplyr v1.1+ and the new base pipe |>
result <- data |>
  filter(year >= 2020) |>
  summarise(
    avg_value = mean(value, na.rm = TRUE),
    n = n(),
    .by = district
  )

# new join syntax
dat <- left_join(df1, df2, by = join_by(id))

# Use across for transformations
dat |>
  mutate(across(where(is.character), ~ as.numeric(str_replace(.x, ",", "."))))

# Avoid superseded or deprecated functions or arguments like:
mutate_at()
select_if()
filter_at()
ggplot() + geom_line(size = 2)
```

#### Option 2: data.table

data.table is extremely fast and memory-efficient. It is ideal for very large datasets. Note that data.table has its own `IDate` class and doesn't work that well with `sf` objects.

```r
# Fast, memory-efficient for large datasets
library(data.table)

dt <- setDT(data)
# Use built-in chaining with []
result <- dt[year >= 2020,
             .(avg_value = mean(value, na.rm = TRUE), n = .N),
             by = district][order(-avg_value)]

# Or, using pipes
result <- dt[year >= 2020,
             .(avg_value = mean(value, na.rm = TRUE), n = .N),
             by = district] |>
  _[order(-avg_value)]
```

#### Option 3: dtplyr (Hybrid)

For those who prefer `dplyr` syntax but need `data.table` performance, `dtplyr` translates `dplyr` verbs to `data.table` operations. Note that not all `dplyr` functions are supported and that performance is not always as good as pure `data.table`.

```r
# dplyr syntax with data.table backend
library(dtplyr)
library(dplyr)
dt <- lazy_dt(data.table(data))
result <- dt |>
  filter(year >= 2020) |>
  summarise(
    avg_value = mean(value, na.rm = TRUE),
    n = n(),
    .by = district
  ) |>
  as.data.table()
```

### Code Style Guide

For most users, we recommend following the [tidyverse style guide](https://style.tidyverse.org/). For help with syntax formatting, use the `styler` package or the [Air](https://www.tidyverse.org/blog/2025/02/air/) add-in.

```r
# Good practices ------------------------------------------------

# 1. Use meaningful variable names
iptu <- read_csv("data/iptu_2024.csv")  # âœ“
df <- read_csv("data/census_2022.csv")  # âœ—

# 2. Comment your code

# Calculate price per square meter adjusting for inflation
iptu <- iptu |>
  mutate(
    price_m2_real = (sale_price / area) * inflation_index
  )

# 3. Use functions for repeated operations
calculate_gini <- function(income_vector) {
  # Gini coefficient calculation
  # Returns value between 0 (equality) and 1 (inequality)
  n <- length(income_vector)
  income_sorted <- sort(income_vector)
  cumsum_income <- cumsum(income_sorted)
  (2 * sum((1:n) * income_sorted)) / (n * sum(income_sorted)) - (n + 1) / n
}

# 4. Use relative paths with here package
library(here)
data <- read_csv(here("data", "census_2022.csv"))  # âœ“
data <- read_csv(here("data/census_2022.csv"))  # âœ“
```

### Performance Tips

```r
# Profile your code
library(profvis)
profvis({
  # Your analysis code here
})

# Use parallel processing for large operations
library(furrr)
plan(multisession, workers = 4)

results <- future_map(datasets, process_function)

# Efficient data reading
library(arrow)
dat <- read_parquet("large_dataset.parquet")  # Much faster than CSV

dat <- vroom::vroom("large_file.csv")  # Fast CSV reading
dat <- data.table::fread("large_file.csv")  # Fast CSV reading

# OBS: in some cases, setting sf::sf_use_s2(FALSE) can speed up spatial operations
# at the cost of some accuracy in geometric calculations. Consider the trade-offs.
sf::sf_use_s2(FALSE)

# OBS2: when working with survey data, using xtabs() and weighted.mean() can be
# much faster than survey functions. However, they don't compute standard errors.
xtabs(wt ~ var1 + var2, data = survey_data)
weighted.mean(survey_data$income, w = survey_data$wt)
```

---

## Data Access Tools

### Database Connections

R offers robust packages for connecting to various databases. Always use environment variables or `.Renviron` files to manage credentials securely.

#### PostgreSQL
```r
library(DBI)
library(RPostgres)

# Secure connection (use .Renviron for credentials)
con <- dbConnect(
  Postgres(),
  dbname = Sys.getenv("DB_NAME"),
  host = Sys.getenv("DB_HOST"),
  port = 5432,
  user = Sys.getenv("DB_USER"),
  password = Sys.getenv("DB_PASSWORD")
)

# Query data
iptu_data <- tbl(con, "iptu_2024") %>%
  filter(district == "PINHEIROS") %>%
  collect()

dbDisconnect(con)
```

#### Google BigQuery
```r
library(bigrquery)

# Authenticate (will open browser)
bq_auth()

# Query public dataset
sql <- "
  SELECT
    pickup_datetime,
    fare_amount,
    pickup_longitude,
    pickup_latitude
  FROM `bigquery-public-data.new_york_taxi.trips`
  WHERE EXTRACT(YEAR FROM pickup_datetime) = 2019
  LIMIT 10000
"

taxi_data <- bq_table_download(
  bq_project_query("your-project-id", sql)
)
```

### API Integration

#### Google Sheets Integration
```r
library(googlesheets4)

# Authenticate
gs4_auth()

# Read public sheet
sheet_url <- "https://docs.google.com/spreadsheets/d/..."
data <- read_sheet(sheet_url)

# Write to private sheet (requires auth)
write_sheet(
  results,
  ss = "your-sheet-id",
  sheet = "Results"
)
```

### Web Scraping (When APIs Unavailable)
```r
library(rvest)
library(polite)

# Always be polite!
session <- bow("https://example.gov.br")

# Scrape responsibly
page <- scrape(session) %>%
  html_nodes(".data-table") %>%
  html_table()

# For JavaScript-heavy sites
library(RSelenium)
driver <- rsDriver(browser = "firefox")
remDr <- driver$client
remDr$navigate("https://interactive-site.com")
```

---

## ðŸ¤– AI-Assisted Research

### Using AI Tools Effectively

Currently, several AI tools can assist with coding and data analysis. Below are some recommendations and workflows for using them effectively in R.

#### 1. Claude (Anthropic) - Best for R code

Claude is generally considered the best AI for coding and has been embraced by the R community. Most practitioners consider it superior to alternatives like ChatGPT for generating R code. Use Claude for:

* Complex data analysis code.
* Debugging R scripts.
* Writing documentation.
* Creating R functions.
* General coding tasks.

If possible, we recommend using Claude Pro for access to ClaudeCode, which is optimized for coding tasks.

**Example workflow:**
```r
# 1. Use Claude to generate initial code
# Prompt: "Write an R function to calculate spatial autocorrelation
#          using Moran's I for SÃ£o Paulo district data"

# 2. Test and validate the code
source("claude_generated_function.R")
result <- calculate_morans_i(data)

# 3. Ask for improvements
# Prompt: "Add bootstrap confidence intervals to this Moran's I function"
```

#### 2. GitHub Copilot

GitHub Copilot is an AI pair programmer that integrates directly into RStudio. It provides real-time code suggestions as you type, making it a great tool for boosting productivity. It's also currently the only AI completion tool that works directly within RStudio.

**Setup in RStudio**

1. Install GitHub Copilot extension
2. Authenticate with GitHub
3. Use Tab to accept suggestions

```r
# Start typing and Copilot will suggest:
# Function to clean Brazilian CPF numbers
clean_cpf <- function(cpf) {
  # Copilot will complete this function
}
```

#### 3. Windsurf - Free alternative to Copilot
Useful free alternative for code suggestions, but less powerful than Copilot. Doesn't work with RStudio.

#### 4. ChatGPT (OpenAI)
Functionalities are the same as Claude, but may require more careful prompting for R code.

### AI Best Practices

```r
# DO: Verify AI-generated code
# Always test with known inputs
test_data <- data.frame(
  value = c(100, 200, 300),
  group = c("A", "A", "B")
)
result <- ai_generated_function(test_data)
stopifnot(all(result$expected == c(150, 150, 300)))

# DO: Use AI for boilerplate
# Let AI write the structure, you add the logic

# DON'T: Blindly trust statistical interpretations
# Always verify statistical claims with textbooks

# DON'T: Share sensitive data
# Use dummy data when asking for help
```

### Prompt Engineering for R

```r
# Effective prompts include:

# 1. Context
"I'm analyzing housing prices in SÃ£o Paulo using hedonic regression"

# 2. Specific requirements
"Using tidyverse and fixest packages"

# 3. Data structure
"My data has columns: price, area_m2, bedrooms, district, year"

# 4. Expected output
"Return a tibble with district-level fixed effects"

# Full example prompt:
"Write an R function using tidyverse that takes a dataframe of
SÃ£o Paulo housing transactions with columns (price, area_m2,
bedrooms, district, year) and returns a hedonic regression of price
per square meter, controlling for district fixed effects using the
fixest package. Include robust standard errors."
```

---

## Insper Resources

### Insper Packages

#### insperplot
```r
# Beautiful, consistent plots following Insper brand
devtools::install_github("portalcidados/insperplot")
library(insperplot)

ggplot(data, aes(x = year, y = value)) +
  geom_line() +
  theme_insper() +
  scale_color_insper()
```

### Insper Data Resources

- **Insper DataLab**: Access to proprietary datasets
- **Insper Dataverse**: https://dataverse.insper.edu.br
- **Portal CiDados**: Urban data portal
- **Insper GitHub**: https://github.com/insper

---

## Essential References

### Core R Books (Free Online)

1. **[R for Data Science (2nd Edition)](https://r4ds.hadley.nz/)**
   - Authors: Wickham, Ã‡etinkaya-Rundel, Grolemund
   - Essential for tidyverse mastery

2. **[Advanced R](https://adv-r.hadley.nz/)**
   - Author: Hadley Wickham
   - Deep dive into R programming

3. **[Geocomputation with R](https://r.geocompx.org/)**
   - Authors: Lovelace, Nowosad, Muenchow
   - Spatial analysis and mapping

4. **[Tidy Modeling with R](https://www.tmwr.org/)**
   - Authors: Kuhn, Silge
   - Modern approach to statistical modeling

5. **[R Graphics Cookbook](https://r-graphics.org/)**
   - Author: Winston Chang
   - Complete guide to ggplot2

### Urban Economics & Econometrics

6. **[Introduction to Econometrics with R](https://www.econometrics-with-r.org/)**
   - Authors: Hanck, Arnold, Gerber, Schmelzer

7. **[Causal Inference: The Mixtape](https://mixtape.scunning.com/)**
   - Author: Scott Cunningham
   - Modern causal inference methods

8. **[The Effect](https://theeffectbook.net/)**
   - Author: Nick Huntington-Klein
   - Causal inference with R examples

### Brazilian Data Resources

- **[IPEA Data](http://www.ipeadata.gov.br/)**: Economic indicators
- **[IBGE](https://www.ibge.gov.br/)**: Census and surveys
- **[CEM USP](https://centrodametropole.fflch.usp.br/)**: Metropolitan data
- **[Seade](https://www.seade.gov.br/)**: SÃ£o Paulo statistics
- **[DataSUS](https://datasus.saude.gov.br/)**: Health data

---

## Reproducibility Checklist

### Before Starting Your Project

- [ ] Create a new RStudio project
- [ ] Initialize git repository
- [ ] Set up renv: `renv::init()`
- [ ] Create folder structure
- [ ] Write initial README
- [ ] Set up .gitignore

### During Development

- [ ] Commit code regularly with meaningful messages
- [ ] Document data sources
- [ ] Comment complex code sections
- [ ] Create functions for repeated tasks
- [ ] Update renv: `renv::snapshot()`

### Data Management

- [ ] Keep raw data immutable
- [ ] Document all transformations
- [ ] Create codebook for variables
- [ ] Version control processed data
- [ ] Validate data quality

### Analysis

- [ ] Set random seed for reproducibility
- [ ] Save intermediate results
- [ ] Create analysis notebooks
- [ ] Document model specifications

### Before Publishing

- [ ] Clean and organize all scripts
- [ ] Test full pipeline from scratch
- [ ] Update documentation
- [ ] Create replication instructions
- [ ] Archive data and code (Dataverse)
- [ ] Generate DOI for citation

### Collaboration

- [ ] Use consistent coding style
- [ ] Review code with colleagues
- [ ] Document dependencies clearly

---

## Quick Start Template

```r
# Create a new urban economics project
create_project <- function(project_name) {
  # Create project
  usethis::create_project(project_name)

  # Set up git
  usethis::use_git()

  # Initialize renv
  renv::init()

  # Create folder structure
  dirs <- c("data/raw", "data/processed", "data/final",
            "R", "notebooks", "outputs/figures",
            "outputs/tables", "outputs/reports", "docs")
  lapply(dirs, dir.create, recursive = TRUE, showWarnings = FALSE)

  # Create README
  usethis::use_readme_md()

  # Create .gitignore
  usethis::use_git_ignore(c(
    "*.Rdata", "*.Rhistory", ".Rproj.user/",
    "data/raw/*", "!data/raw/README.md"
  ))

  # Install packages
  install.packages(c(
    "tidyverse", "data.table", "fixest", "modelsummary",
    "sf", "arrow", "janitor", "here"
  ))

  message("Project '", project_name, "' created successfully!")

}

# Use it:
create_project("housing_analysis_sp")
```
---

## Support

For questions about these guidelines or urban data science at Insper:

- **Email**: cidades@insper.edu.br
- **Office Hours**: Schedule at [link]
- **Workshops**: Check calendar for R training sessions

---

*Last updated: 2025-01-10 | Version 1.0*
